{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.1: Exploring the green reds\n",
    "\n",
    "a) plot a scatterplot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"input/winequality-red.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_matrix(data, good_threshold, bad_threshold, save_plot=False, quality=True):\n",
    "    \n",
    "    names = data.columns\n",
    "    good_data = data.loc[data['quality'] >= good_threshold]\n",
    "    bad_data = data.loc[data['quality'] <= bad_threshold]\n",
    "\n",
    "    if quality==False:\n",
    "        good_data = good_data.drop('quality', 1)\n",
    "        bad_data = bad_data.drop('quality', 1)\n",
    "        names = good_data.columns\n",
    "\n",
    "    \n",
    "    numvars, numdata = good_data.shape\n",
    "    fig, axes = plt.subplots(nrows=numdata, ncols=numdata, figsize=(20,20))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        # Hide all ticks and labels\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "    # Plot the data.\n",
    "    for i in range(0, numdata):\n",
    "        for j in range(0, numdata):\n",
    "            if (i != j):\n",
    "                for x, y in [(i,j), (j,i)]:\n",
    "                    #iloc allows me to grab by index so when I'm plotting it just uses the column index and plots them.\n",
    "                    axes[x,y].plot(bad_data.iloc[:, y], bad_data.iloc[:, x], linestyle='none', marker='.', color='red', mfc='none')\n",
    "                    axes[x,y].plot(good_data.iloc[:, y], good_data.iloc[:, x], linestyle='none', marker='.', color='green', mfc='none')\n",
    "\n",
    "    # Label the diagonal subplots...\n",
    "    for i, label in enumerate(names):\n",
    "        axes[i,i].annotate(label, (0.5, 0.5), xycoords='axes fraction',\n",
    "                ha='center', va='center')\n",
    "    if (save_plot == True):\n",
    "        plt.savefig('scatter_plot.png')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_threshold = 8\n",
    "bad_threshold = 3\n",
    "\n",
    "fig = plot_scatter_matrix(data,good_threshold,bad_threshold,save_plot=False)\n",
    "fig.suptitle('Simple Scatterplot Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Observations:\n",
    " 1-Dimensional Analysis\n",
    " \n",
    "    \n",
    "    1) Lower volatile acidity trended to higher quality\n",
    "    2) Low chlorides led trended to higher quality\n",
    "    3) Higher alcohol content trended to higher quality\n",
    "    4) Higher sulphates trended to higher quality\n",
    "    5) Lower density trended to higher quality\n",
    "    6) Lower pH trended to higher quality\n",
    "    7) Most low quality wines had low citric acid\n",
    "    8) Lower Sulfer dioxide trended to lower quality\n",
    " \n",
    " 2-Dimensional Analysis\n",
    "    * plots with greater separation between green and red show that these two factors have a relationship to quality \n",
    "    * poolings of green or red are also good indicators to relationship of quality\n",
    "    \n",
    "    1) Higher alcohol content with low volatile acidity led to higher ratings\n",
    "    2) Lower Density and higher citric acid showed a trended to higher ratings\n",
    "    3) High citric acid and low chlorides showed higher ratings\n",
    "    \n",
    "I think that volatile acidity, chlorids, sulphates, and alcohol content will be the most useful factors in deciding quality of wine because of the severity of their grouping and large disparity between high and low wines.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.2 Learning to Perceptron\n",
    "\n",
    "# a) Implement a perceptron that:\n",
    "    \n",
    "       * Has randomly initialized weights and bias\n",
    "       * Uses the Rosenblatt perceptron learning rule (with changeable learning rate)\n",
    "       * Utilizes the heaviside step activation function (discrete version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions\n",
    "\n",
    "Formula for activation function:\n",
    "    * activation = (weight(0) * X(0)) + (weight(n) * x(n)) + bias\n",
    "\n",
    "Heaviside step activation function:\n",
    "    * if activation is > 0 then it predicts 1 if else returns 0.\n",
    "    * simple binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        #iterate through inputs and multiply them to corresponding weights\n",
    "        activation += weights[i + 1] * row[i]\n",
    "        \n",
    "    #use a transfer function to change value to 1 or 0.\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Implement a function to train your perceptron\n",
    "\n",
    "    • Have a way to specify number of training epochs\n",
    "    • Train your perceptron until it makes no errors, if training epochs is set to 0,\n",
    "    • Have a way to specify learning rate.\n",
    "    • Return a list of python tuples containing (performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "There are 3 loops we need to perform in the function:\n",
    "\n",
    "    1) Loop over each epoch.\n",
    "    2) Loop over each row in the training data per epoch.\n",
    "    3) Loop over each weight per feature in row.\n",
    "    4) Calculate the prediction with the respective weights to input\n",
    "    5) Recalculate the weights using this function:\n",
    "            New Weight = Old Weight + (l_rate * (x) * error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "\n",
    "#The perceptron learning rule basically just adds the actual value \n",
    "def train_perceptron(train, l_rate, n_epoch):\n",
    "    performance = []\n",
    "\n",
    "    #random weights\n",
    "    random.seed(1)\n",
    "    weights = [random.uniform(0, 1) for i in range(len(train[0]))]\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += abs(error)\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row) - 1):\n",
    "                weights[i + 1] = weights[i + 1] + (l_rate * row[i] * error)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        performance.append((epoch, sum_error, weights[1:], weights[0]))\n",
    "    if n_epoch <= 0:\n",
    "        epoch = 0\n",
    "        sum_error = 1.0\n",
    "        while sum_error != 0:\n",
    "            sum_error = 0.0\n",
    "            for row in train:\n",
    "                prediction = predict(row, weights)\n",
    "                error = row[-1] - prediction\n",
    "                sum_error += abs(error)\n",
    "                weights[0] = weights[0] + l_rate * error\n",
    "                for i in range(len(row) - 1):\n",
    "                    weights[i + 1] = weights[i + 1] + (l_rate * row[i] * error)\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))            \n",
    "            performance.append((epoch, sum_error, weights[1:], weights[0]))\n",
    "            epoch += 1 \n",
    "    print(weights)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_list(data):\n",
    "        #convert to list\n",
    "        return data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataframe(data, good_threshold, bad_threshold, columns, set_range=False):\n",
    "    new_data = data.copy()\n",
    "\n",
    "    if (set_range==False):\n",
    "        #Covert all labels to 1 or 0 with threshold of good and bad\n",
    "        new_data['good_bad'] = [1 if x >= good_threshold else 0 for x in new_data['quality']]\n",
    "    else:\n",
    "        #Covert all labels to 1 or -1 with threshold of good and bad\n",
    "        new_data['good_bad'] = [1 if x >= good_threshold else -1 for x in new_data['quality']]\n",
    "    \n",
    "    #Grab all data higher than 8 or lower than 3 in quality.\n",
    "    new_data = new_data.loc[(data['quality'] >= good_threshold) | (data['quality'] <= bad_threshold)]\n",
    "    \n",
    "    #Grab only the specified columns\n",
    "    new_data = new_data.loc[:, columns]\n",
    "    \n",
    "    new_data = new_data.reset_index(drop=True)\n",
    "    return(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = parse_dataframe(data, good_threshold=8, bad_threshold=3, columns=['pH', 'alcohol', 'good_bad'])\n",
    "test_train = dataframe_to_list(test_train)\n",
    "test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = train_perceptron(test_train, l_rate=5, n_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in test_train:\n",
    "#     prediction = predict(row, weights)\n",
    "#     print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format (epoch, error, weights, bias)\n",
    "print(performance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Plotting performance\n",
    "    \n",
    "    • The first plot should plot the number of errors your perceptron made as a function of epoch. Be careful with how you calculate errors!\n",
    "    • The second plot should plot the decision boundary of your perceptron and also show ‘good’ and ‘bad’ wine data points on the final training epoch. This second plot should also shade ‘good’ and ‘bad’ areas!\n",
    "    • Your function should allow the user to specify a specific epoch and see what the decision boundary of the perceptron was on that epoch. If a negative epoch is given, cause the plots to show the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump\n",
    "\n",
    "def eval_y_per_x(formula, x_values):\n",
    "    y_values = []\n",
    "    for x in x_values:\n",
    "        y_values.append(eval(formula))\n",
    "    return y_values\n",
    "\n",
    "def graph(formula, wine_data, ax2):     \n",
    "    x = wine_data.alcohol.tolist()\n",
    "    x = [i for i in frange(min(x), max(x), .1)]\n",
    "    y = eval_y_per_x(formula, x)\n",
    "    d_min = [min(y) for l in range(0, len(x))]\n",
    "    d_max = max(y)\n",
    "    ax2.plot(x, y, linestyle='dashed', label='decision boundary')\n",
    "    ax2.fill_between(x, y, d_min, facecolor='lightgreen', alpha=0.5)\n",
    "    ax2.fill_between(x, y, d_max, facecolor='pink', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if epoch is equal to -1 then it shows the line for last epoch\n",
    "#please put a valid epoch under the number of epoch's attempted\n",
    "\n",
    "def plot_performance(performance, wine_data, good_thresh, bad_thresh, epoch, save_plot=False):\n",
    "    #Separate Data to good and bad wine depending on threshold\n",
    "    good_data = wine_data.loc[data['quality'] >= good_thresh]\n",
    "    bad_data = wine_data.loc[data['quality'] <= bad_thresh]\n",
    "   \n",
    "    listofepochs = [x[0] for x in performance]\n",
    "    error = [x[1] for x in performance]\n",
    "    \n",
    "    #find length of epoch's so I can use last one if -1\n",
    "    if (epoch >= len(listofepochs)):\n",
    "        print(\"Please Put Valid Epoch. If you wish for last epoch put -1\")\n",
    "        return\n",
    "    \n",
    "    if (epoch < 0):\n",
    "        epoch = len(listofepochs) - 1\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "    ax1.plot(listofepochs, error)\n",
    "    ax1.set_title('Error / Epoch Graph')\n",
    "    ax1.set_ylabel('Sum_Error')\n",
    "    ax1.set_xlabel('Epoch #')\n",
    "\n",
    "    ax2.scatter(good_data.alcohol.tolist(), good_data.pH.tolist(), color = 'green', label='good')\n",
    "    ax2.scatter(bad_data.alcohol.tolist(), bad_data.pH.tolist(), color = 'red', label='bad')\n",
    "    formula = '({}*x + {})/-{}'.format(performance[epoch][2][1], performance[epoch][3], performance[epoch][2][0])\n",
    "\n",
    "    graph(formula, wine_data, ax2)\n",
    "    ax2.set_title('Decision boundary on epoch: {}'.format(epoch))\n",
    "    ax2.set_ylabel('pH')\n",
    "    ax2.set_xlabel('alcohol')\n",
    "    ax2.legend()\n",
    "    if (save_plot == True):\n",
    "        plt.savefig('batchsize_35_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance, data, good_thresh=8, bad_thresh=3, epoch=10, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) The reason it takes so many epochs to train is because the data is not normalized (or represented between 0 and 1). It takes alot longer for the bias and weights to calibrate for numbers that are larger than one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Function for normalizing is ((X - min) / (max - min))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = data.copy()\n",
    "features = [x for x in norm_data.columns]\n",
    "for feature in features:\n",
    "    maxim = norm_data[feature].max()\n",
    "    minim = norm_data[feature].min()\n",
    "    print(feature, maxim, minim)\n",
    "    if feature != 'quality':\n",
    "        norm_data[feature] = norm_data[feature].apply(lambda x: (x - minim) / (maxim - minim))\n",
    "    \n",
    "# #Double Check if it was normalized\n",
    "# norm_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train1 = parse_dataframe(norm_data, 8, 3, ['pH', 'alcohol', 'good_bad'])\n",
    "norm_train1 = dataframe_to_list(norm_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 5\n",
    "n_epoch = 0\n",
    "performance = train_perceptron(norm_train1, l_rate, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance, norm_data, 8, 3, -1, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.3 My fair ADALINE\n",
    "\n",
    "# a) Results after lowering good threshold to 7 and raising bad threshold to 4\n",
    "       \n",
    "       * Look at the graph below and see how interlaced both datasets are. There is no way to make a linear classifier that can differentiate between the two without errors\n",
    "       * Technical term is linearly inseparable problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_new_threshold = parse_dataframe(norm_data, good_threshold=7, bad_threshold=4, columns=['pH', 'alcohol', 'good_bad'], set_range=True)\n",
    "norm_train_new_threshold = dataframe_to_list(norm_train_new_threshold)\n",
    "print(len(norm_train_new_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = train_perceptron(norm_train_new_threshold, l_rate=.1215, n_epoch=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance, norm_data, good_thresh=7, bad_thresh=4, epoch=-1, save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Implement an ADALINE that:\n",
    "\n",
    "    • Has randomly initialized weights and bias\n",
    "    • Uses a linear activation function and some kind of quantizer\n",
    "    • Uses the Widrow-Hoff learning rule\n",
    "\n",
    " Quantizer = converts activation to 1 or 0 based off the value\n",
    " \n",
    " Widrow-Hoff learning = uses activation value instead of a threshold value for error calculation\n",
    " \n",
    " \n",
    " Stochastic Gradient Descent\n",
    "    \n",
    "    Gradient Descent is the process of minimizing error by following the gradients of the cost function.\n",
    "   \n",
    " Similarities between perceptron and adaline model:\n",
    "\n",
    "    * they are both binary classifiers\n",
    "    * both have a linear decision boundary\n",
    "    * both can learn iteratively, sample by sample\n",
    "    * both use a threshold function\n",
    "    \n",
    " Difference between perceptron and adaline model\n",
    "    \n",
    "    * both are class\n",
    "    * activation functions returns continuous values or floats between 0 and 1 that are more like probablities\n",
    "    * These continuous values can be used to learn how close or far this prediction was to the correct value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_continuous(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        #iterate through inputs and multiply them to corresponding weights\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return activation, 1.0 if activation >= 0.0 else -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Create a training function for adaline\n",
    "\n",
    "    • Take in your red wine data as a parameter\n",
    "    • Have a way to specify number of training epochs\n",
    "    • If training epochs is set to 0, your ADALINE should train until it converges\n",
    "    on a good set of weights.\n",
    "    • Have a way to specify learning rate.\n",
    "    • Have an option to perform either online learning or batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_aladine(train, l_rate, n_epoch, batch=1):\n",
    "\n",
    "    #random weights\n",
    "    random.seed(2)\n",
    "    weights = [random.uniform(0, 1) for i in range(len(train[0]))]\n",
    "    \n",
    "    previous_error = 99999\n",
    "    sum_error = 0\n",
    "    performance = []\n",
    "    batch_counter = 0\n",
    "    error = 0.0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            batch_counter += 1\n",
    "            probability, prediction = predict_continuous(row, weights)\n",
    "            error += row[-1] - probability\n",
    "            sum_error += abs(row[-1] - prediction)\n",
    "#             print(row[-1], prediction, sum_error)\n",
    "            if (batch_counter == batch):\n",
    "                batch_counter = 0\n",
    "                weights[0] = weights[0] + l_rate * error\n",
    "                for i in range(len(row) - 1):\n",
    "                    weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "                error = 0\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        performance.append((epoch, sum_error, weights[1:], weights[0]))\n",
    "    if n_epoch <= 0:\n",
    "        epoch = 0\n",
    "        while (previous_error >= sum_error):\n",
    "            if (epoch != 0):\n",
    "                previous_error = sum_error\n",
    "            sum_error = 0.0\n",
    "            for row in train:\n",
    "                batch_counter += 1\n",
    "                probability, prediction = predict_continuous(row, weights)\n",
    "                error += row[-1] - probability\n",
    "                sum_error += abs(row[-1] - prediction)\n",
    "                if (batch_counter == batch):\n",
    "                    batch_counter = 0\n",
    "                    weights[0] = weights[0] + l_rate * error\n",
    "                    for i in range(len(row) - 1):\n",
    "                        weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "                error = 0\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))            \n",
    "            performance.append((epoch, sum_error, weights[1:], weights[0]))\n",
    "            epoch += 1\n",
    "            \n",
    "    print(weights)\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train3 = parse_dataframe(norm_data, good_threshold=7, bad_threshold=4, columns=['pH', 'alcohol', 'good_bad'], set_range=True)\n",
    "norm_train3 = dataframe_to_list(norm_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = train_aladine(norm_train3, l_rate=.006, n_epoch=400, batch=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance, norm_data, good_thresh=7, bad_thresh=4, epoch=-1, save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias = performance[-1][3]\n",
    "# weights = list(performance[-1][2])\n",
    "# print(bias, weights)\n",
    "\n",
    "# weights.insert(0, bias)\n",
    "# print(weights)\n",
    "\n",
    "# for row in norm_train3:\n",
    "#     probability, prediction = predict(row, weights)\n",
    "#     print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "    \n",
    "    batch size = 1, learning rate = .00212, epoch = 1000\n",
    "        these hyperparameters actually did very well but didn't give me my best results because the batchsize only allows for one single data points to change the weights ,so the line can't pass into the center of the plots and grab more red and blues despite a few single errors\n",
    "        \n",
    "    batch size = 15, learning rate = .00212, epoch = 1000\n",
    "        this did the best it was a good balance of constant learning rate and a solid batch size that allows the line to move into the mass without too much concern for error over each individual input value.\n",
    "        \n",
    "    batch size = 35, learning_rate = 0.006, epoch = 1000\n",
    "        these settings did okay but the decision line began to get lost within the plot and looks like it couldn't converge on a good set of weights because of how large the batches are. lowest error I got was about 90 with these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.4 Advanced wine sampling and resampling\n",
    "\n",
    "a) Write a function that uses the holdout method to partition the red wine data into a\n",
    "training and a validation set. The function should take a parameter to adjust the\n",
    "proportion of training to validation data. It should return a tuple containing:\n",
    "(training_pandas_dataframe, validation_pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# california_housing_dataframe = california_housing_dataframe.reindex(\n",
    "#     np.random.permutation(california_housing_dataframe.index))\n",
    "\n",
    "def split_train_validation(data,percent_train_data):\n",
    "    if (percent_train_data >= 1 or percent_train_data <= 0):\n",
    "        raise Warning(\"The percentage must be between 0 and 1\")\n",
    "    train_data = data.iloc[0:round(len(data)*percent_train_data), :]\n",
    "    validation_data = data.iloc[round(len(data)*percent_train_data):, :]\n",
    "    return(train_data, validation_data)\n",
    "\n",
    "\n",
    "train_data, validation_data = split_train_validation(norm_data, percent_train_data=0.8)\n",
    "print(\"Total_data {}, training_data {}, validation_data {}\".format(len(norm_data), len(train_data), len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write a function that generates a k-fold cross validation dataset.\n",
    "\n",
    "What is k-fold? It is a way of creating training and validation while using your whole data set.\n",
    "The K stands for the number of groups to split the data set. A proper k-fold cross validation will separate 1 group into validation and the rest into training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_dataset(data, k, randomize=False):\n",
    "    if (k == 0):\n",
    "        raise Warning(\"the k-fold is 0!\")\n",
    "    groups = []\n",
    "    previous_row = 0\n",
    "    training_data = pd.DataFrame()\n",
    "    return_values = []\n",
    "    \n",
    "    if (randomize == True):\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #split into k groups\n",
    "    for i in range(1, k+1):\n",
    "        index = round(len(data)/k*i)\n",
    "        groups.append(data.iloc[previous_row: index,:])\n",
    "        previous_row = index\n",
    "    for i in range(len(groups)):\n",
    "        training_data = pd.DataFrame()\n",
    "        validation_data = groups[i]\n",
    "        for x in range(len(groups)):\n",
    "            if x != i:\n",
    "                training_data = training_data.append(groups[x])\n",
    "        return_values.append((training_data, validation_data))\n",
    "    return (return_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train4 = parse_dataframe(norm_data, good_threshold=7, bad_threshold=4, columns=['pH', 'alcohol', 'good_bad'], set_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = k_fold_dataset(norm_train4, 10, randomize=True)\n",
    "count = 0\n",
    "print(len(norm_train4))\n",
    "for i,x in a:\n",
    "    count +=1\n",
    "    print (i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What effects does changing learning rate and number of training epochs have on the\n",
    "    ADALINE when evaluated via k-fold cross-validation? To address this question,\n",
    "    you should write (or modify) a function that will train and assess the ADALINE\n",
    "    on each training and cross-validation fold produced by your k-fold function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model with different k folds\n",
    "#So i have to train of training_data then I have to run a percent correct on the test data as an evaluation tool\n",
    "#Write a function that will train on each training and cross validation fold. Basically a for loop that will go through training data\n",
    "#I should give both the validation and training to the function and compare the losses for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights_with_validation(train, validation, l_rate, n_epoch, batch=1):\n",
    "    performance = []\n",
    "    batch_counter = 0\n",
    "    error = 0.0\n",
    "    #random weights\n",
    "    random.seed(1)\n",
    "    weights = [random.uniform(0, 1) for i in range(len(train[0]))]\n",
    "    \n",
    "    previous_error = 99999\n",
    "    sum_train_error = 0\n",
    "    sum_valid_error = 0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        #reset errors for each epoch.\n",
    "        sum_train_error = 0.0\n",
    "        sum_valid_error = 0.0\n",
    "\n",
    "        #Make predictions per row and calculate error to update after each batch\n",
    "        for row in train:\n",
    "            batch_counter += 1\n",
    "            probability, prediction = predict_continuous(row, weights)\n",
    "            error += row[-1] - probability\n",
    "            sum_train_error += abs(row[-1] - prediction)\n",
    "            if (batch_counter == batch):\n",
    "                batch_counter = 0\n",
    "                weights[0] = weights[0] + l_rate * error\n",
    "                for i in range(len(row) - 1):\n",
    "                    weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "                error = 0\n",
    "                \n",
    "        #Make predicitons for validation to do evaluation of model\n",
    "        for row in validation:\n",
    "            probability, prediction = predict_continuous(row, weights)\n",
    "            sum_valid_error += abs(row[-1] - prediction)\n",
    "        \n",
    "        #print performance\n",
    "#         print('>epoch=%d, lrate=%.3f, train_error=%.3f, valid_error=%.3f' % (epoch, l_rate, sum_train_error/len(train), sum_valid_error/len(validation)))\n",
    "        performance.append((epoch, sum_train_error, sum_valid_error/len(validation), weights[1:], weights[0]))\n",
    "    if n_epoch <= 0:\n",
    "        epoch = 0\n",
    "        while (previous_error >= sum_train_error):\n",
    "            #Training continues until the error begins to go back up.\n",
    "            if (epoch != 0):\n",
    "                previous_error = sum_train_error\n",
    "                \n",
    "            #reset errors for each epoch.\n",
    "            sum_train_error = 0.0\n",
    "            sum_valid_error = 0.0\n",
    "            \n",
    "            #Make predictions per row and calculate error to update after each batch\n",
    "            for row in train:\n",
    "                batch_counter += 1\n",
    "                probability, prediction = predict_continuous(row, weights)\n",
    "                error += row[-1] - probability\n",
    "                sum_error += abs(row[-1] - prediction)\n",
    "                if (batch_counter == batch):\n",
    "                    batch_counter = 0\n",
    "                    weights[0] = weights[0] + l_rate * error\n",
    "                    for i in range(len(row) - 1):\n",
    "                        weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "                error = 0\n",
    "            \n",
    "            #Make predicitons for validation to do evaluation of model\n",
    "            for row in validation:\n",
    "                probability, prediction = predict_continuous(row, weights)\n",
    "                sum_valid_error += abs(row[-1] - prediction)\n",
    "            \n",
    "#             print('>epoch=%d, lrate=%.3f, train_error=%.3f, valid_error=%.3f' % (epoch, l_rate, sum_train_error/len(train), sum_valid_error/len(validation)))\n",
    "            performance.append((epoch, sum_train_error, sum_valid_error/len(validation), weights[1:], weights[0]))\n",
    "            epoch += 1\n",
    "            \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "sum_valid_error = 0.0\n",
    "\n",
    "for (train, valid) in a:\n",
    "    print(\"{}-Fold Cross Validation\".format(count))\n",
    "    train = dataframe_to_list(train)\n",
    "    valid = dataframe_to_list(valid)\n",
    "    performance = train_weights_with_validation(train, valid, l_rate=.5, n_epoch=100, batch=1)\n",
    "    valid_error = performance[-1][2]\n",
    "    print(\"Valid Error : {}\".format(valid_error))\n",
    "    sum_valid_error += valid_error\n",
    "    count += 1\n",
    "print(sum_valid_error/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) observations: \n",
    "\n",
    "    * When I changed the learning rate to .02 from .2 the validation error dropped down from .3785 to .3071 which is a huge improvement. If I put the learning rate at .5 the validation error goes up to .42857 because it trains too quickly on the dataset and doesn't fine tune the weights.\n",
    "    \n",
    "    * larger epoch number led to larger validation error because of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train5 = parse_dataframe(norm_data, 7, 4, ['fixed acidity', 'volatile acidity', 'citric acid',\n",
    "       'chlorides', 'density', 'pH', 'sulphates', 'alcohol', 'good_bad'])\n",
    "norm_train5 = dataframe_to_list(norm_train5)\n",
    "performance = train_perceptron(norm_train5, l_rate=.01, n_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train5 = parse_dataframe(norm_data, 7, 4, ['fixed acidity', 'volatile acidity', 'citric acid',\n",
    "       'chlorides', 'density', 'pH', 'sulphates', 'alcohol', 'good_bad'], set_range=True)\n",
    "norm_train5 = dataframe_to_list(norm_train5)\n",
    "performance = train_aladine(norm_train5, l_rate=.05, n_epoch=75, batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias = performance[-1][3]\n",
    "# weights = list(performance[-1][2])\n",
    "# print(bias, weights)\n",
    "\n",
    "# weights.insert(0, bias)\n",
    "\n",
    "# for row in norm_train5:\n",
    "#     prediction = predict(row, weights)\n",
    "#     print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "\n",
    "    * It trains well on all the data for the perceptron model ,but the aladine is having trouble. With all the data and a learning rate of .01 the model trains decently and get around 28 wrong in a dataset of 280.\n",
    "    * Aladine doesn't produce as good outputs on the same dataset as the perceptron, but it does train if I put n_epoch at 30 and l_rate at .05 for batch size 10 or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "\n",
    "    * For 1 dimensions it is a point on a line\n",
    "    * For 2 dimensions it is a line on a point\n",
    "    * For 3 dimensions it is a plane in a 3 dimensional space\n",
    "    * I can't visually conceptualize beyond 3 dimensions lol. \n",
    "      https://www.youtube.com/watch?v=JkxieS-6WuA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V.6 Marvin’s rebuttal\n",
    "\n",
    "a) While not a wine. . . find a way to successfully classify the Pan-Galactic Gargle Blaster\n",
    "dataset. Show that your perceptron or ADALINE successfully classifies the PanGalactic\n",
    "Gargle Blaster data set by plotting the decision boundary and also show\n",
    "‘good’ and ‘bad’ Gargle Blaster data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gargle_blaster = pd.read_csv(\"input/Pan Galactic Gargle Blaster.csv\")\n",
    "gargle_blaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_matrix(gargle_blaster, 8, 3, quality=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the dataset\n",
    "Put all the data points about the origin (0, 0) and then square everything\n",
    "This will allow for me to fit a linear equation to divide the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_gargle = gargle_blaster.copy()\n",
    "features = [x for x in norm_gargle.columns]\n",
    "for feature in features:\n",
    "    maxim = max(norm_gargle[feature])\n",
    "    minim = min(norm_gargle[feature])\n",
    "    print(feature, maxim, minim)\n",
    "    if feature != 'quality':\n",
    "        norm_gargle[feature] = norm_gargle[feature].apply(lambda x: (x - minim) / (maxim - minim))\n",
    "        norm_gargle[feature] = norm_gargle[feature].apply(lambda x : (x - 0.5)**2)\n",
    "    \n",
    "# #Double Check if it was normalized\n",
    "# norm_gargle.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_gargle.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(gargle_blaster, good_thresh, bad_thresh):\n",
    "    #Separate Data to good and bad wine depending on threshold\n",
    "    good_data = gargle_blaster.loc[(gargle_blaster['quality']) >= good_thresh]\n",
    "    bad_data = gargle_blaster.loc[(gargle_blaster['quality']) <= bad_thresh ]\n",
    "   \n",
    "    fig, ax2 = plt.subplots(figsize=(4, 4), ncols=1)\n",
    "\n",
    "\n",
    "    ax2.plot(good_data.iloc[:, 0], good_data.iloc[:, 1], linestyle='none', marker='.', color='green', mfc='none', label='good')\n",
    "    ax2.plot(bad_data.iloc[:, 0], bad_data.iloc[:, 1], linestyle='none', marker='.', color='red', mfc='none', label='bad')\n",
    "\n",
    "    ax2.set_ylabel('wonderflonium')\n",
    "    ax2.set_xlabel('fallian marsh gas')\n",
    "    ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(norm_gargle, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gargle_train = parse_dataframe(norm_gargle, 6, 5, ['wonderflonium', 'fallian marsh gas','good_bad'])\n",
    "gargle_train = dataframe_to_list(gargle_train)\n",
    "gargle_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = train_perceptron(gargle_train, l_rate=.505, n_epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gargle(formula, gargle, ax2):     \n",
    "    x = gargle['fallian marsh gas'].tolist()\n",
    "    x = [i for i in frange(min(x), max(x), .1)]\n",
    "    y = eval_y_per_x(formula, x)\n",
    "    d_min = [min(y) for l in range(0, len(x))]\n",
    "    d_max = max(y)\n",
    "    ax2.plot(x, y, linestyle='dashed', label='decision boundary')\n",
    "    ax2.fill_between(x, y, d_min, facecolor='lightgreen', alpha=0.5)\n",
    "    ax2.fill_between(x, y, d_max, facecolor='pink', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if epoch is equal to -1 then it shows the line for last epoch\n",
    "#please put a valid epoch under the number of epoch's attempted\n",
    "import math \n",
    "\n",
    "def plot_gargle(performance, gargle_blaster, good_thresh, bad_thresh, epoch, save_plot=False):\n",
    "    #Separate Data to good and bad wine depending on threshold\n",
    "    good_data = gargle_blaster.loc[(gargle_blaster['quality']) >= good_thresh]\n",
    "    bad_data = gargle_blaster.loc[(gargle_blaster['quality']) <= bad_thresh ]\n",
    "   \n",
    "    listofepochs = [x[0] for x in performance]\n",
    "    error = [x[1] for x in performance]\n",
    "    \n",
    "    #find length of epoch's so I can use last one if -1\n",
    "    if (epoch >= len(listofepochs)):\n",
    "        print(\"Please Put Valid Epoch. If you wish for last epoch put -1\")\n",
    "        return\n",
    "    \n",
    "    if (epoch < 0):\n",
    "        epoch = len(listofepochs) - 1\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "    ax1.plot(listofepochs, error)\n",
    "    ax1.set_title('Error / Epoch Graph')\n",
    "    ax1.set_ylabel('Sum_Error')\n",
    "    ax1.set_xlabel('Epoch #')\n",
    "\n",
    "    ax2.plot(good_data.iloc[:, 0], good_data.iloc[:, 1], linestyle='none', marker='.', color='green', mfc='none', label='good')\n",
    "    ax2.plot(bad_data.iloc[:, 0], bad_data.iloc[:, 1], linestyle='none', marker='.', color='red', mfc='none', label='bad')\n",
    "    \n",
    "    formula = '({}*x + {})/-{}'.format(performance[epoch][2][1], performance[epoch][3], performance[epoch][2][0])\n",
    "\n",
    "    graph_gargle(formula, gargle_blaster, ax2)\n",
    "    ax2.set_title('Decision boundary on epoch: {}'.format(epoch))\n",
    "    ax2.set_ylabel('wonderflonium')\n",
    "    ax2.set_xlabel('fallian marsh gas')\n",
    "    ax2.legend()\n",
    "    if (save_plot == True):\n",
    "        plt.savefig('gargle_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gargle_blaster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gargle(performance, norm_gargle, good_thresh=6, bad_thresh=5, epoch=-1, save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
